{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed Forward Neural Network 실습\n",
    "\n",
    "## Up Down Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample data 생성\n",
    "# feature 10개의 숫자가 오름차순[0, 1, ... , 9]이면 1로 내림차순[9, 8, ..., 0]이면 0으로 labeling\n",
    "# sample의 총 갯수는 train 1000개, test 100개\n",
    "\n",
    "samples = 1000\n",
    "test_samples = 100\n",
    "train_dataset = './ffnn_dataset/train_dataset.csv'\n",
    "test_dataset = './ffnn_dataset/test_dataset.csv'\n",
    "\n",
    "def write_dataset(samples, test_samples, train_dir, test_dir):\n",
    "    up = [i for i in range(10)]\n",
    "    down = [9-i for i in range(10)]\n",
    "\n",
    "    data = []\n",
    "    label = []\n",
    "    for i in range(samples):\n",
    "        data.append(up)\n",
    "        data.append(down)\n",
    "        label.append([1])\n",
    "        label.append([0])\n",
    "\n",
    "    with open(train_dataset, 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for i in range(samples-test_samples):\n",
    "            writer.writerow(label[i] + data[i])\n",
    "        print('train data is written')\n",
    "\n",
    "    with open(test_dataset, 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for i in range(test_samples):\n",
    "            writer.writerow(label[i] + data[i])\n",
    "        print('test data is written')\n",
    "        \n",
    "#write_dataset(1000, 100, train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이용을 강추하는 부분\n",
    "train_dataset = tf.contrib.data.TextLineDataset(train_dataset)\n",
    "train_dataset = train_dataset.batch(32)\n",
    "#dataset = dataset.shuffle(7777)\n",
    "train_dataset = train_dataset.repeat(100)\n",
    "\n",
    "# 데코레이터!\n",
    "# dataset = tf.contrib.data.TextLineDataset(train_dataset).shuffle(7777).batch(32) \n",
    "# 혹은 method 별로 끊어서 다음 라인과 같이 사용하는 것도 가능\n",
    "# dataset = tf.contrib.data.TextLineDataset(train_dataset)\n",
    "# dataset = dataset.batch(32)\n",
    "# dataset = dataset.shuffle(7777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test dataset도 마찬가지 방법으로 불러오기\n",
    "test_dataset = tf.contrib.data.TextLineDataset(test_dataset)\n",
    "test_dataset = test_dataset.batch(32)\n",
    "#dataset = dataset.shuffle(7777)\n",
    "test_dataset = test_dataset.repeat(99999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_itr = train_dataset.make_one_shot_iterator()\n",
    "test_itr = test_dataset.make_one_shot_iterator()\n",
    "\n",
    "train_batch = train_itr.get_next()\n",
    "test_batch = test_itr.get_next()\n",
    "\n",
    "# record_default는 missing data 발생 시 이를 채워주는 default value를 설정하는 것\n",
    "train_decoded_batch = tf.decode_csv(train_batch, record_defaults=[[0]]*11)\n",
    "test_decoded_batch = tf.decode_csv(test_batch, record_defaults=[[0]]*11)\n",
    "\n",
    "train_label = tf.reshape(train_decoded_batch[0], [-1, 1])\n",
    "#label = tf.expand_dims(decoded_batch[0], axis=-1)\n",
    "\n",
    "train_feature =  tf.stack(train_decoded_batch[1:], axis=1)\n",
    "#원래의 데이터를 row로 묶어서 하나의 sample data로 만들지 col.으로 묶어서 하나의 sample로 만들지 데이터를 보고 선택!\n",
    "#row_feature = tf.stack(decoded_batch[1:], axis=0)\n",
    "#col_feature = tf.stack(decoded_batch[1:], axis=1)\n",
    "\n",
    "test_label = tf.reshape(test_decoded_batch[0], [-1, 1])\n",
    "test_feature =  tf.stack(test_decoded_batch[1:], axis=1)\n",
    "\n",
    "train_label = tf.cast(train_label, tf.float32)\n",
    "train_feature =tf.cast(train_feature, tf.float32)\n",
    "\n",
    "test_label = tf.cast(test_label, tf.float32)\n",
    "test_feature =tf.cast(test_feature, tf.float32)\n",
    "\n",
    "print(train_label)\n",
    "print(train_feature)\n",
    "print(test_label)\n",
    "print(test_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    _train_label, _train_feature, _test_label, _test_feature = sess.run([train_label, train_feature, test_label, test_feature])        \n",
    "    # 출력하면 아래의 각 변수들은 batch size만큼의 sample data들을 가지고 있음\n",
    "    print(_train_label)\n",
    "    print(_train_feature)\n",
    "    print(_test_label)\n",
    "    print(_test_feature)\n",
    "    print(_train_label.shape, _train_feature.shape)\n",
    "    print(_test_label.shape, _test_feature.shape)\n",
    "    \n",
    "print(_train_label.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 본격적으로 모델구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # arguments가 많으니 하나씩, 차근차근\n",
    "# layer1 = tf.layers.dense(train_feature, units=5)\n",
    "# layer2 = tf.layers.dense(layer1, units=6)\n",
    "# layer3 = tf.layers.dense(layer2, units=7)\n",
    "# layer4 = tf.layers.dense(layer3, units=8)\n",
    "# out = tf.layers.dense(layer4, units=1)\n",
    "\n",
    "# # 디버깅을 위해 layer shape을 살펴보기 위함\n",
    "# print('layer1 shape {}'.format(layer1.shape))\n",
    "# print('layer2 shape {}'.format(layer2.shape))\n",
    "# print('layer3 shape {}'.format(layer3.shape))\n",
    "# print('layer4 shape {}'.format(layer4.shape))\n",
    "# print('out shape {}'.format(out.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test dataset을 이용해서 학습의 loss가 줄어들고 있는지 확인 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_model(x, activation, reuse=False):\n",
    "    layer1 = tf.layers.dense(x, units=5, activation=activation, reuse=reuse, name='layer1')\n",
    "    layer2 = tf.layers.dense(layer1, units=6, activation=activation, reuse=reuse, name='layer2')\n",
    "    layer3 = tf.layers.dense(layer2, units=7, activation=activation, reuse=reuse, name='layer3')\n",
    "    layer4 = tf.layers.dense(layer3, units=8, activation=activation, reuse=reuse, name='layer4')\n",
    "    out = tf.layers.dense(layer4, units=1, activation=activation, reuse=reuse, name='out')\n",
    "    \n",
    "    print('layer1 shape {}'.format(layer1.shape))\n",
    "    print('layer2 shape {}'.format(layer2.shape))\n",
    "    print('layer3 shape {}'.format(layer3.shape))\n",
    "    print('layer4 shape {}'.format(layer4.shape))\n",
    "    print('out shape {}'.format(out.shape))\n",
    "    \n",
    "    return out\n",
    "\n",
    "    # 디버깅을 위해 layer shape을 살펴보기 위함\n",
    "\n",
    "    \n",
    "train_out = bin_model(train_feature, activation=tf.nn.sigmoid)\n",
    "test_out = bin_model(test_feature, activation=tf.nn.sigmoid, reuse=True)\n",
    "\n",
    "for v in tf.trainable_variables():\n",
    "    print(v)\n",
    "    \n",
    "pred = tf.nn.sigmoid(test_out)\n",
    "\n",
    "# 검증을 위한 accuracy 도출: 참고로 검증을 위한 methods는 metrics에 있음\n",
    "accuracy = tf.metrics.accuracy(test_label, tf.round(pred))\n",
    "\n",
    "loss = tf.losses.sigmoid_cross_entropy(train_label, train_out)\n",
    "train_op = tf.train.GradientDescentOptimizer(1e-2).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    for i in range(1000):\n",
    "        _, _loss, _acc = sess.run([train_op, loss, accuracy])\n",
    "        if i%100 == 0:\n",
    "            _pred = sess.run(pred)\n",
    "            print('step: {}, loss: {}, acc: {}'.format(i, _loss, _acc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/beomyongnho/.pyenv/versions/3.6.1/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-34ab8a214bcd>:36: TextLineDataset.__init__ (from tensorflow.contrib.data.python.ops.readers) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.TextLineDataset`.\n",
      "layer1 shape (?, 5)\n",
      "layer2 shape (?, 6)\n",
      "layer3 shape (?, 7)\n",
      "layer4 shape (?, 8)\n",
      "out shape (?, 1)\n",
      "layer1 shape (?, 5)\n",
      "layer2 shape (?, 6)\n",
      "layer3 shape (?, 7)\n",
      "layer4 shape (?, 8)\n",
      "out shape (?, 1)\n",
      "<tf.Variable 'layer1/kernel:0' shape=(10, 5) dtype=float32_ref>\n",
      "<tf.Variable 'layer1/bias:0' shape=(5,) dtype=float32_ref>\n",
      "<tf.Variable 'layer2/kernel:0' shape=(5, 6) dtype=float32_ref>\n",
      "<tf.Variable 'layer2/bias:0' shape=(6,) dtype=float32_ref>\n",
      "<tf.Variable 'layer3/kernel:0' shape=(6, 7) dtype=float32_ref>\n",
      "<tf.Variable 'layer3/bias:0' shape=(7,) dtype=float32_ref>\n",
      "<tf.Variable 'layer4/kernel:0' shape=(7, 8) dtype=float32_ref>\n",
      "<tf.Variable 'layer4/bias:0' shape=(8,) dtype=float32_ref>\n",
      "<tf.Variable 'out/kernel:0' shape=(8, 1) dtype=float32_ref>\n",
      "<tf.Variable 'out/bias:0' shape=(1,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "samples = 1000\n",
    "test_samples = 100\n",
    "train_dataset_dir = './ffnn_dataset/train_dataset.csv'\n",
    "test_dataset_dir = './ffnn_dataset/test_dataset.csv'\n",
    "\n",
    "def write_dataset(samples, test_samples, train_dir, test_dir):\n",
    "    up = [i for i in range(10)]\n",
    "    down = [9-i for i in range(10)]\n",
    "\n",
    "    data = []\n",
    "    label = []\n",
    "    for i in range(samples):\n",
    "        data.append(up)\n",
    "        data.append(down)\n",
    "        label.append([1])\n",
    "        label.append([0])\n",
    "\n",
    "    with open(train_dataset, 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for i in range(samples-test_samples):\n",
    "            writer.writerow(label[i] + data[i])\n",
    "        print('train data is written')\n",
    "\n",
    "    with open(test_dataset, 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for i in range(test_samples):\n",
    "            writer.writerow(label[i] + data[i])\n",
    "        print('test data is written')\n",
    "        \n",
    "# write_dataset(1000, 100, train_dataset, test_dataset)\n",
    "\n",
    "train_dataset = tf.contrib.data.TextLineDataset(train_dataset_dir)\n",
    "train_dataset = train_dataset.batch(32)\n",
    "train_dataset = train_dataset.repeat(100)\n",
    "\n",
    "test_dataset = tf.contrib.data.TextLineDataset(test_dataset_dir)\n",
    "test_dataset = test_dataset.batch(32)\n",
    "test_dataset = test_dataset.repeat(999999)\n",
    "\n",
    "train_itr = train_dataset.make_one_shot_iterator()\n",
    "test_itr = test_dataset.make_one_shot_iterator()\n",
    "\n",
    "train_batch = train_itr.get_next()\n",
    "test_batch = test_itr.get_next()\n",
    "\n",
    "train_batch = tf.decode_csv(train_batch, record_defaults=[[0]]*11)\n",
    "test_batch = tf.decode_csv(test_batch, record_defaults=[[0]]*11)\n",
    "\n",
    "train_label = tf.reshape(train_batch[0], [-1, 1])\n",
    "# label = tf.expand_dims(train_batch[0], axis=-1)\n",
    "train_feature = tf.stack(train_batch[1:], axis=1)\n",
    "\n",
    "test_label = tf.reshape(test_batch[0], [-1, 1])\n",
    "# label = tf.expand_dims(test_batch[0], axis=-1)\n",
    "test_feature = tf.stack(test_batch[1:], axis=1)\n",
    "\n",
    "train_label = tf.cast(train_label, tf.float32)\n",
    "train_feature = tf.cast(train_feature, tf.float32)\n",
    "test_label = tf.cast(test_label, tf.float32)\n",
    "test_feature = tf.cast(test_feature, tf.float32)\n",
    "\n",
    "def bin_model(x, activation=None, reuse=False):\n",
    "    layer1 = tf.layers.dense(x, units=5, activation=activation, reuse=reuse, name='layer1')\n",
    "    layer2 = tf.layers.dense(layer1, units=6, activation=activation, reuse=reuse, name='layer2')\n",
    "    layer3 = tf.layers.dense(layer2, units=7, activation=activation, reuse=reuse, name='layer3')\n",
    "    layer4 = tf.layers.dense(layer3, units=8, activation=activation, reuse=reuse, name='layer4')\n",
    "    out = tf.layers.dense(layer4, units=1, reuse=reuse, name='out')\n",
    "    print('layer1 shape {}'.format(layer1.shape))\n",
    "    print('layer2 shape {}'.format(layer2.shape))\n",
    "    print('layer3 shape {}'.format(layer3.shape))\n",
    "    print('layer4 shape {}'.format(layer4.shape))\n",
    "    print('out shape {}'.format(out.shape))\n",
    "    return out\n",
    "\n",
    "train_out = bin_model(train_feature, activation=tf.nn.sigmoid) \n",
    "test_out = bin_model(test_feature, activation=tf.nn.sigmoid, reuse=True)\n",
    "\n",
    "for v in tf.trainable_variables():\n",
    "    print(v)\n",
    "    \n",
    "# variable이 모두 define되는 시점에서 saver를 만든다.\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "\n",
    "pred = tf.nn.sigmoid(test_out)\n",
    "accuracy = tf.metrics.accuracy(test_label, tf.round(pred))\n",
    "\n",
    "loss = tf.losses.sigmoid_cross_entropy(train_label, train_out)\n",
    "train_op = tf.train.GradientDescentOptimizer(1e-2).minimize(loss)\n",
    "\n",
    "tf.summary.scalar('loss', loss)\n",
    "\n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 0.6946947574615479, acc: 0.0\n",
      "step: 100, loss: 0.6933138370513916, acc: 0.5\n",
      "step: 200, loss: 0.6931241750717163, acc: 0.5\n",
      "step: 300, loss: 0.693089485168457, acc: 0.5\n",
      "step: 400, loss: 0.6930748820304871, acc: 0.5\n",
      "step: 500, loss: 0.6930627822875977, acc: 0.5\n",
      "step: 600, loss: 0.6930509805679321, acc: 0.5\n",
      "step: 700, loss: 0.693039059638977, acc: 0.5\n",
      "step: 800, loss: 0.6930269598960876, acc: 0.5\n",
      "step: 900, loss: 0.6930146217346191, acc: 0.5\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    writer = tf.summary.FileWriter('./ffnn_logs/', sess.graph)\n",
    "    for i in range(1000):\n",
    "        _, _loss, _acc, _summ = sess.run([train_op, loss, accuracy, merged])\n",
    "        writer.add_summary(_summ, i)\n",
    "        if i%100 == 0:\n",
    "            _pred = sess.run(pred)\n",
    "            print('step: {}, loss: {}, acc: {}'.format(i, _loss, _acc[0]))\n",
    "    saver.save(sess, './ffnn_logs/ffnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./ffnn_logs/ffnn\n",
      "[[ 0.49987361]\n",
      " [ 0.49972823]\n",
      " [ 0.49987361]\n",
      " [ 0.49972823]\n",
      " [ 0.49987361]\n",
      " [ 0.49972823]\n",
      " [ 0.49987361]\n",
      " [ 0.49972823]\n",
      " [ 0.49987361]\n",
      " [ 0.49972823]\n",
      " [ 0.49987361]\n",
      " [ 0.49972823]\n",
      " [ 0.49987361]\n",
      " [ 0.49972823]\n",
      " [ 0.49987361]\n",
      " [ 0.49972823]\n",
      " [ 0.49987361]\n",
      " [ 0.49972823]\n",
      " [ 0.49987361]\n",
      " [ 0.49972823]\n",
      " [ 0.49987361]\n",
      " [ 0.49972823]\n",
      " [ 0.49987361]\n",
      " [ 0.49972823]\n",
      " [ 0.49987361]\n",
      " [ 0.49972823]\n",
      " [ 0.49987361]\n",
      " [ 0.49972823]\n",
      " [ 0.49987361]\n",
      " [ 0.49972823]\n",
      " [ 0.49987361]\n",
      " [ 0.49972823]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, './ffnn_logs/ffnn')\n",
    "    _pred = sess.run(pred)\n",
    "    print(_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tensorboard 실행을 위한 쉘 명령어\n",
    "# tensorboard -logdir=/path/of/your/logs #1.4.1버젼\n",
    "# tensorboard --logdir=/path/of/your/logs #1.3.1버젼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 실행할 때마다 그래프가 계속 생성되는 것을 방지하기 위해서는 다음 코드를 사용하여 리셋\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
